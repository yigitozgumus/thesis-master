%!TEX root = ../2019_7_Ozgumus_Semsi_Yigit.tex

\begingroup

This chapter presents the modifications applied to aforementioned approaches to improve the performance 
of the anomaly detection. All the discussed models measures its performance metric on well known datasets, 
such as CIFAR-10 \cite{cifar10} and SVHN \cite{Netzer2011ReadingDI}. The dataset used in this thesis presents 
additional challenges to this problem we aim to solve. The first section will introduce its dataset to the 
reader and will give examples. Next section will discuss the shortcomings of the previous approaches regarding 
the interpretation of the dataset and detection of the existent anomalies. Sections \ref{sec:encebgan} and 
\ref{sec:sencebgan} will explain the modified architecture and the significance of the changes.

\section{SEM Image Dataset}
\label{sec:sem}

Nanofibrous materials acquired increasingly significant demand from variety of fields in the Industry. 
It constitutes a foundation material for a lot of products including areas in medicine, filtration, sensors 
and manufacturing applications. \cite{carrera2016defect}. Despite the demand and continuous research 
development towards its production, manufacturing nanofibrous materials is still  challenge for scaled of 
mass production. Several techniques for producing nanofibers have been presented in the literature. 
\cite{carrera2016defect}. Electrospinning method is the focus of this anomaly detection task. It produces 
a structure which consists of filaments woven in with randomized geometric pattern. You can see one of the images 
of the material produced without any anomalies in figure \ref{fig:data_norm}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.75\textwidth]{dataset}
	\caption{Part of Training Dataset from the SEM Image Dataset \cite{sem}}
	\label{fig:data_norm}
\end{figure}

Dataset consists of 5 images with no anomalies and 40 sample images that have various types of anomalous regions. To preserve 
computational efficiency without sacrifising from performance, training dataset and testing dataset is sampled from SEM image dataset.
\begin{figure}[h!]
	 \subfloat[Normal regions]{
		\begin{minipage}[c][0.8\width]{0.5\textwidth}
			\centering
			\fbox{\includegraphics[width=1\textwidth]{sample_normal}}
			\label{fig:data_sample_normal}
	\end{minipage}}
	\hspace*{\fill}%
	\subfloat[Anomalous regions]{
		\begin{minipage}[c][0.8\width]{0.5\textwidth}
			\centering
			\fbox{\includegraphics[width=1\textwidth]{sample_anomaly}}
			\label{fig:data_sample_anomaly}
	\end{minipage}}
	\caption{Normal and Anomalous region patches for the training and testing}
	\label{fig:data_samples}
\end{figure}

$32 \times 32$ patches are selected as the image size as if proposed framework prove useful for the anomaly detection, 
testing the model with other known datasets such as CIFAR10 \cite{cifar10} and SVHN \cite{Netzer2011ReadingDI} would be more convenient. 
$28 \times 28$ patch size is also considered but choosing $28 \times 28$ image size forced model to have less transposed convolution 
layers (see chapter \ref{chap:imp_details} for model details) and GAN models designed with that architecture experienced model collapse 
frequently in the preliminary experiments.
Figure \ref{fig:data_sample_normal} shows the example patches used for the 
training phase. Training dataset consists of patches which contains no anomalies. Figure \ref{fig:data_sample_anomaly} 
shows the anomalous samples which are used in the inference stage. The Anomalous regions can reside in both the topmost layer
or can be hidden in deep in the woven structure.

\section{Analysis of Aforementioned Approaches}
\label{sec:analysis_before}

This section presents an analysis on the performance of GAN based state of the art anomaly detection methods 
presented in section \ref{sec:gan_based_sota} and a discussion to identify the shortcomings of these methods. 
Stabilization issues in the adversarial training stage, the method to calculate the anomaly score, and reconstruction 
issues related to the encoding of latent representation will be discussed.

Proposed methods in chapter \ref{chap:sota} can be further divided into 2 seperate categories with respect to their generator structure. 
\begin{itemize}
	\item Pure GAN ( AnoGAN (\ref{sec:anogan}), BiGAN (\ref{sec:bigan}) and ALAD (\ref{sec:alad}))
	\item Autoencoder Variants (GANomaly and Skip-GANomaly (\ref{sec:ganomaly}))
\end{itemize}

First group has a generator that accepts the noise as an input and uses adversarial training to match the generated sample distribution 
to the input data distribution. The latter uses an autoencoder based decoder behalf of the generator but still employs adversarial training
to learn the latent representation. During the analysis of the first two observations, autoencoder variants will not be considered for discussion 
since their generator network does not suffer from the stabilization issues of GANs.

\subsection{Stabilization of Adversarial Training}

Stabilization of loss functions of generator and discriminator is still an important issue in GAN training. 
Some works in literature proposed additional stabilization "tricks" to improve the convergence properties of the objective function 
and prevent mode collapse (\cite{methods,fm}). These include adding noise with a decay over time to both input image and generated 
sample before putting through discriminator to add a factor of robustness to the discriminator, using soft labels to define the true 
and fake member class instead of the binary truth values and flipping labels of the true and generated images to fool the discriminator even more
and provide higher gradient flow to generator early on in the training to help it learn to generate images better \cite{fm}.

In the training phase of an generative adversarial network, loss values of generator does not portray a traditional convergence line in plots.
Adversarial minimax game prevents the losses of the player networks to converge to a certain limit. If the loss is reaching zero in any case, it indicates 
a problem in the learning capacity of one of the networks. If discriminator loss converges to zero too quickly, it means that it learned to 
discriminate between the real images and the ones generated by the generator network. On the other hand if the generator loss converges to zero 
early in the training it indicates a mode collapse. In some cases generator generates an image that perfectly fools the discriminator early in the 
training. If gradients obtained from this loss computation is high enough, generator might stop learning from the gradient flow of discriminator 
and start to generate the same sample in each iteration. This sample also may not be visually similar to the target distribution. This eliminates the 
purpose of having a generator network. This scenerio is called mode collapse. 

To mitigate these shortcomings, ablation study is performed on all Pure GAN models which consists of previously mentioned training 
improvements. Even though training graph shows convincing progress, reconstructions obtained from the generator are also considered 
for inspection. Quantative results will be discussed in chapter \ref{chap:expres}.

% Figure with bigan alad anogan graphs and reconstructions

%%%


\subsection{Convergence of Encoder Training}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.75\textwidth]{reconstruct_ganomaly}
	\caption{SENCEBGAN Model Overview }
	\label{fig:sencebgan_model}
\end{figure}

\subsection{Encoded Energy Based Generative Adversarial Network}
\label{sec:encebgan}

\begin{figure}[h!]
	\centering
	\includegraphics[width=1.0\textwidth]{encebgan}
	\caption{ENCEBGAN Model Overview }
	\label{fig:encebgan_model}
\end{figure}

\subsection{Anomaly Score Computation}


\section{Sequentially Encoded Energy Based Generative Adversarial Network}
\label{sec:sencebgan}

\begin{figure}[h!]
	\centering
	\includegraphics[width=1.0\textwidth]{sencebgan}
	\caption{SENCEBGAN Model Overview }
	\label{fig:sencebgan_model}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.75\textwidth]{reconstruct_sencebgan}
	\caption{SENCEBGAN Image Reconstruction  }
	\label{fig:sencebgan_model}
\end{figure}



\endgroup